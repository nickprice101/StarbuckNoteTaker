# --- Imports ---
import os
import numpy as np
import tensorflow as tf
from keras_nlp.models import BertClassifier, BertBackbone
from tensorflow.keras.layers import TextVectorization
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
import torch
from torch.utils.data import Dataset
import re
from collections import Counter
from training_data import category_examples

# --- Paths ---
OUT_DIR  = "flan_t5_small_type_first_tflite_min"
ASSETS_DIR = os.path.join(OUT_DIR, "assets")
os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(ASSETS_DIR, exist_ok=True)
android_asset_dir = r"C:\Users\nickp\AppData\Local\Programs\Microsoft VS Code\app\src\main\assets"
os.makedirs(android_asset_dir, exist_ok=True)

# Prepare training data for T5 summary generation with TRUE abstractive summaries
notes = []
summaries = []

# Helper function to create actual abstractive summaries
def create_abstractive_summary(note_text, category):
    """Generate a truly condensed summary, not just a reformatting"""
    category_friendly = category.replace("_", " ").title()
    
    # Extract key information
    if ":" in note_text:
        context, content = note_text.split(":", 1)
        context = context.strip()
        content = content.strip()
    else:
        context = "note"
        content = note_text.strip()
    
    # Create category-specific abstractive summaries
    if category == "SHOPPING_LIST":
        items = [item.strip() for item in content.split(',')]
        # Clean up items to get main product names
        clean_items = []
        for item in items[:3]:  # Only use first 3 items
            # Extract key product name (first 1-2 words)
            words = item.split()
            if len(words) > 0:
                # Remove quantities and measurements
                product = ' '.join([w for w in words[:2] if not any(char.isdigit() for char in w)])
                if product and product.lower() not in ['need', 'get', 'buy', 'pick']:
                    clean_items.append(product)
        
        if len(clean_items) == 0:
            return "Shopping list"
        elif len(clean_items) == 1:
            return f"Shopping for {clean_items[0]}"
        elif len(items) <= 3:
            return f"Shopping for {', '.join(clean_items)}"
        else:
            return f"Shopping for {', '.join(clean_items[:2])} and more"
    
    elif category == "MEETING_RECAP":
        if "assigned" in content.lower():
            return f"Meeting discussed tasks and assignments"
        else:
            return f"Meeting covered {context.lower()} topics"
    
    elif category == "TECHNICAL_REFERENCE":
        if len(content) > 50:
            return f"Technical procedure for {context.lower()}"
        else:
            return f"{category_friendly} note about {context.lower()}"
    
    elif category == "FOOD_RECIPE":
        if len(content) > 100:
            return f"Recipe with detailed {context.lower()} instructions"
        elif len(content) > 30:
            return f"Recipe for {context.lower()}"
        else:
            return f"Recipe note: {content[:30]}"
    
    elif category == "TRAVEL_LOG":
        # Extract destination from context
        if 'to' in context.lower():
            parts = context.lower().split('to')
            if len(parts) > 1:
                destination = parts[-1].strip().split()[0]  # Get first word of destination
                return f"Travel to {destination.title()}"
        # Fallback: extract location from content
        words = content.split()
        for i, word in enumerate(words[:10]):
            if word[0].isupper() and word.lower() not in ['woke', 'packed', 'noted']:
                return f"Travel experience in {word}"
        return "Travel log entry"
    
    elif category == "WORK_PROJECT":
        return f"Project update on {context.lower()}"
    
    elif category == "REMINDER":
        return f"Reminder to {content.split(',')[0][:40] if ',' in content else content[:40]}"
    
    elif category == "FINANCE_LEGAL":
        return f"Financial activity: {context.lower()}"
    
    elif category == "HEALTH_WELLNESS":
        return f"Health tracking: {context.lower()}"
    
    elif category == "EDUCATION_LEARNING":
        return f"Learning activity: {context.lower()}"
    
    elif category == "CREATIVE_WRITING":
        return f"Creative writing: {context.lower()}"
    
    elif category == "PERSONAL_DAILY_LIFE":
        return f"Daily activity: {context.lower()}"
    
    elif category == "HOME_FAMILY":
        return f"Family/home note: {context.lower()}"
    
    elif category == "SELF_IMPROVEMENT":
        return f"Personal development: {context.lower()}"
    
    else:
        # Generic condensed summary
        words = content.split()
        if len(words) > 10:
            return f"{category_friendly}: {' '.join(words[:8])}..."
        else:
            return f"{category_friendly}: {context.lower()}"

for category, entries in category_examples.items():
    for entry in entries:
        note_text = entry["note"]
        notes.append(note_text)
        # Create TRUE abstractive summary (condensed, not just reformatted)
        abstractive_summary = create_abstractive_summary(note_text, category)
        summaries.append(abstractive_summary)

# Load T5 tokenizer and model
MODEL_ID = "google/flan-t5-small"
tokenizer = T5Tokenizer.from_pretrained(MODEL_ID)
model = T5ForConditionalGeneration.from_pretrained(MODEL_ID)

# Tokenize inputs and targets
MAX_INPUT_LEN = 64
MAX_TARGET_LEN = 32
inputs = tokenizer(notes, padding="max_length", truncation=True, max_length=MAX_INPUT_LEN, return_tensors="tf")
targets = tokenizer(summaries, padding="max_length", truncation=True, max_length=MAX_TARGET_LEN, return_tensors="tf")

# Prepare labels (replace pad token id with -100 for loss masking)
labels = targets.input_ids.numpy()
labels[labels == tokenizer.pad_token_id] = -100

# PyTorch training using Hugging Face Trainer API
class T5NoteSummaryDataset(Dataset):
    def __init__(self, input_ids, attention_mask, labels):
        self.input_ids = torch.tensor(input_ids)
        self.attention_mask = torch.tensor(attention_mask)
        self.labels = torch.tensor(labels, dtype=torch.long)
    def __len__(self):
        return len(self.input_ids)
    def __getitem__(self, idx):
        return {
            "input_ids": self.input_ids[idx],
            "attention_mask": self.attention_mask[idx],
            "labels": self.labels[idx]
        }

train_dataset = T5NoteSummaryDataset(
    inputs.input_ids,
    inputs.attention_mask,
    labels
)

training_args = TrainingArguments(
    output_dir="./results_t5_note_summary",
    num_train_epochs=8,  # Increased from 3 to 8 for better summarization learning
    per_device_train_batch_size=2,
    save_steps=10,
    save_total_limit=2,
    logging_steps=5,
    report_to=[]
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset
)
trainer.train()

# Export T5 model and tokenizer
EXPORT_DIR = os.path.join(ASSETS_DIR, "t5_note_summary_saved_model")
model.save_pretrained(EXPORT_DIR)
tokenizer.save_pretrained(EXPORT_DIR)

# Convert T5 SavedModel to TFLite (experimental)
try:
    converter = tf.lite.TFLiteConverter.from_saved_model(EXPORT_DIR)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    tflite_t5_path = os.path.join(android_asset_dir, "note_summary_t5.tflite")
    tflite_t5_model = converter.convert()
    with open(tflite_t5_path, "wb") as f:
        f.write(tflite_t5_model)
    print("TFLite T5 model saved to:", tflite_t5_path)
except Exception as e:
    print("TFLite conversion for T5 failed (expected for encoder-decoder models):", e)

# --- Example usage: Generate outputs with the trained model ---
def generate_summary(input_text):
    inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True)
    output_ids = model.generate(
        **inputs, 
        max_length=64,
        no_repeat_ngram_size=3,  # Prevent repeating 3-word sequences
        repetition_penalty=1.5,   # Penalize word repetition
        num_beams=4,              # Use beam search
        early_stopping=True
    )
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

# Minimal KerasNLP classifier pipeline for mobile deployment
# (imports already at top of script)

OUT_DIR  = "flan_t5_small_type_first_tflite_min"
ASSETS_DIR = os.path.join(OUT_DIR, "assets")
os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(ASSETS_DIR, exist_ok=True)

print("TF:", tf.__version__)
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    print(f"GPUs detected: {[gpu.name for gpu in gpus]}")
else:
    print("No GPU detected. Running on CPU.")

# Synthetic training data for note classification
NOTE_TYPES = [
    "PERSONAL_DAILY_LIFE",
    "FINANCE_LEGAL",
    "SELF_IMPROVEMENT",
    "HEALTH_WELLNESS",
    "EDUCATION_LEARNING",
    "HOME_FAMILY",
    "WORK_PROJECT",
    "MEETING_RECAP",
    "SHOPPING_LIST",
    "REMINDER",
    "TRAVEL_LOG",
    "FOOD_RECIPE",
    "CREATIVE_WRITING",
    "TECHNICAL_REFERENCE"
]


note_titles = []
note_labels = []
for idx, category in enumerate(NOTE_TYPES):
    if category in category_examples:
        category_count = 0
        for entry in category_examples[category]:
            note_titles.append(entry["note"])
            note_labels.append(idx)
            category_count += 1
        print(f"Category {idx}: {category} - {category_count} examples")
    else:
        print(f"WARNING: Category {idx}: {category} - NOT FOUND in category_examples!")

print(f"\nTotal training examples: {len(note_titles)}")
print(f"Total categories: {len(NOTE_TYPES)}")
print()

# KerasNLP-based pipeline
bert_backbone = BertBackbone.from_preset("bert_tiny_en_uncased")
classifier = BertClassifier(backbone=bert_backbone, num_classes=len(NOTE_TYPES))

# Use Keras TextVectorization for preprocessing
max_tokens = 10000
output_sequence_length = 32
vectorizer = TextVectorization(max_tokens=max_tokens, output_sequence_length=output_sequence_length)
vectorizer.adapt(note_titles)
train_token_ids = vectorizer(np.array(note_titles)).numpy()

# Build segment_ids and attention_mask for BERT
segment_ids = np.zeros_like(train_token_ids)
attention_mask = (train_token_ids != 0).astype("int32")

# Prepare input dict for classifier
train_inputs = {
    "token_ids": train_token_ids,
    "segment_ids": segment_ids,
    "padding_mask": attention_mask
}
train_labels = np.array(note_labels)

classifier.compile(
    loss="sparse_categorical_crossentropy",
    optimizer="adam",
    metrics=["accuracy"]
)
classifier.fit(train_inputs, train_labels, epochs=20, batch_size=2)

# Export SavedModel
saved_model_dir = os.path.join(ASSETS_DIR, "note_classifier_saved_model")
classifier.export(saved_model_dir)
print("Saved classifier model to:", saved_model_dir)

# Convert SavedModel to TensorFlow Lite
import tensorflow as tf
android_asset_dir = r"C:\Users\nickp\AppData\Local\Programs\Microsoft VS Code\app\src\main\assets"
os.makedirs(android_asset_dir, exist_ok=True)
tflite_path = os.path.join(android_asset_dir, "note_classifier.tflite")
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
with open(tflite_path, "wb") as f:
    f.write(tflite_model)

# --- Example predictions and model validation ---
example_notes = [
    "Reminder: submit Bean Stock enrollment by 05/15 and verify beneficiaries in Workday before the deadline.",
    "Recipe test: crafted honey lavender cold foam, steeped 2 tbsp lavender buds in 1 cup cream, blended with 1 oz honey syrup, tasted best on cold brew.",
    "Family calendar: coordinated with sister Lena about hosting Mother's Day brunch, assigned grocery pickups, reserved family table at local cafe.",
    "Project Voyager: mapped out rollout timeline for new cold foam station, assigned hardware checks to Malik, drafted training deck outline.",
    "Budget review session: reconciled March cafe tips with bank deposit, set aside $120 for partner appreciation gifts, flagged IRS letter needing signature by Friday.",
    "Morning check-in: Woke at 6:30am for a jog around Green Lake, logged 3.2 miles. Packed healthy lunch of quinoa bowl and prepped cold brew concentrate for tomorrow.",
    "Mental health note: used Calm breathing track during break, journaled about guest interaction stress, plan to discuss with therapist on Thursday.",
    "Training recap: completed Origin Espresso module, passed quiz with 92%, shared tasting notes comparing Verona vs Espresso Roast with team."
]

print("\n" + "="*80)
print("CLASSIFIER MODEL PREDICTIONS")
print("="*80 + "\n")

example_token_ids = vectorizer(np.array(example_notes)).numpy()
example_segment_ids = np.zeros_like(example_token_ids)
example_padding_mask = (example_token_ids != 0).astype("int32")
example_inputs = {
    "token_ids": example_token_ids,
    "segment_ids": example_segment_ids,
    "padding_mask": example_padding_mask
}

preds = classifier.predict(example_inputs)
for i, note in enumerate(example_notes):
    pred_class = int(np.argmax(preds[i])) if preds.ndim > 1 else int(preds[i])
    print(f"Input: {note}")
    print(f"Predicted class index: {pred_class}")
    print(f"Predicted category: {NOTE_TYPES[pred_class]}")
    print()

print("\n" + "="*80)
print("T5 SUMMARY GENERATION - FULL [CONTENT TYPE] [CONTEXT] [SUBJECT] FORMAT")
print("="*80 + "\n")

# Generate formatted summaries using the trained T5 model
for i, note in enumerate(example_notes):
    pred_class = int(np.argmax(preds[i])) if preds.ndim > 1 else int(preds[i])
    predicted_category = NOTE_TYPES[pred_class]
    
    # Generate T5 summary
    t5_summary = generate_summary(note)
    
    print(f"Example {i+1}:")
    print(f"  Input Note: {note}")
    print(f"  Predicted Category: {predicted_category}")
    print(f"  T5 Generated Summary: {t5_summary}")
    print(f"  Expected Format: [CONTENT_TYPE] [CONTEXT] [SUBJECT]")
    print()

print("\n" + "="*80)
print("COMPLETE MODEL VALIDATION - INTEGRATED OUTPUT")
print("="*80 + "\n")

# ============================================================================
# ADVANCED SUMMARIZATION COMPONENTS
# ============================================================================

def extract_named_entities(text):
    """
    Extract named entities (people, places, organizations, dates, numbers)
    using pattern matching and heuristics
    """
    entities = {
        'people': [],
        'places': [],
        'dates': [],
        'numbers': [],
        'organizations': []
    }
    
    # Extract capitalized words (potential proper nouns)
    capitalized_words = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', text)
    
    # Common first names (simplified)
    common_names = ['Riley', 'Malik', 'Sarah', 'Eli', 'Jayden', 'Lena', 'Jorge', 
                    'Emma', 'Alicia', 'Iris', 'Marcus', 'Javier', 'Maya']
    
    # Places and organizations
    place_keywords = ['Seattle', 'Portland', 'Austin', 'Kyoto', 'Paris', 'Barcelona',
                     'Olympia', 'Lake', 'Market', 'Hotel', 'Airport']
    org_keywords = ['Starbucks', 'Academy', 'Global', 'Bean Stock', 'Workday', 
                   'SharePoint', 'FigJam', 'IRS', 'LegalHub']
    
    for word in capitalized_words:
        if any(name in word for name in common_names):
            entities['people'].append(word)
        elif any(place in word for place in place_keywords):
            entities['places'].append(word)
        elif any(org in word for org in org_keywords):
            entities['organizations'].append(word)
    
    # Extract dates and times
    date_patterns = [
        r'\b\d{1,2}/\d{1,2}/\d{2,4}\b',  # MM/DD/YYYY
        r'\b\d{1,2}/\d{1,2}\b',           # MM/DD
        r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{1,2}\b',
        r'\b\d{1,2}:\d{2}(?:am|pm)?\b'    # Time
    ]
    for pattern in date_patterns:
        entities['dates'].extend(re.findall(pattern, text, re.IGNORECASE))
    
    # Extract numbers with context (quantities, measurements, percentages)
    number_patterns = [
        r'\b\d+(?:\.\d+)?\s*(?:miles|oz|tbsp|cups?|hours?|minutes?|%|percent|dollars?|\$)\b',
        r'\$\d+(?:\.\d{2})?',
        r'\b\d+\s*cartons?\b',
        r'\b\d+%\b'
    ]
    for pattern in number_patterns:
        entities['numbers'].extend(re.findall(pattern, text, re.IGNORECASE))
    
    return entities

def analyze_sentiment(text, category):
    """
    Analyze sentiment and tone of the note
    """
    # Positive indicators
    positive_words = ['great', 'excellent', 'perfect', 'success', 'achieved', 'completed',
                     'improved', 'praised', 'favorite', 'best', 'wonderful', 'happy',
                     'confidence boost', 'growth', 'enjoyed']
    
    # Negative indicators
    negative_words = ['failed', 'issue', 'problem', 'delayed', 'concern', 'stress',
                     'difficult', 'challenge', 'strain', 'tension', 'discrepancy']
    
    # Neutral/informational indicators
    neutral_words = ['noted', 'documented', 'scheduled', 'updated', 'reviewed',
                    'aligned', 'coordinated', 'confirmed']
    
    text_lower = text.lower()
    
    positive_count = sum(1 for word in positive_words if word in text_lower)
    negative_count = sum(1 for word in negative_words if word in text_lower)
    neutral_count = sum(1 for word in neutral_words if word in text_lower)
    
    # Determine overall sentiment
    if positive_count > negative_count and positive_count > 0:
        sentiment = "positive"
        tone = "optimistic"
    elif negative_count > positive_count and negative_count > 0:
        sentiment = "negative"
        tone = "concerned"
    else:
        sentiment = "neutral"
        tone = "informational"
    
    # Category-specific tone adjustments
    if category in ["REMINDER", "SHOPPING_LIST"]:
        tone = "actionable"
    elif category == "CREATIVE_WRITING":
        tone = "expressive"
    elif category == "TECHNICAL_REFERENCE":
        tone = "instructional"
    
    return {
        'sentiment': sentiment,
        'tone': tone,
        'confidence': max(positive_count, negative_count, neutral_count)
    }

def extract_key_phrases(text, max_phrases=3):
    """
    Extract key phrases using frequency and importance heuristics
    """
    # Remove common stopwords
    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',
                'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
                'should', 'could', 'may', 'might', 'must', 'can', 'this', 'that'}
    
    # Extract noun phrases (simplified - sequences of capitalized or significant words)
    words = text.split()
    
    # Score words by importance
    word_scores = {}
    for i, word in enumerate(words):
        clean_word = re.sub(r'[^\w\s]', '', word.lower())
        if clean_word and clean_word not in stopwords and len(clean_word) > 3:
            # Higher score for capitalized words, numbers, and position
            score = 1
            if word[0].isupper() and i > 0:  # Proper nouns
                score += 2
            if re.search(r'\d', word):  # Contains numbers
                score += 1
            if i < len(words) * 0.3:  # Early in text
                score += 1
            
            word_scores[clean_word] = word_scores.get(clean_word, 0) + score
    
    # Get top phrases
    top_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:max_phrases * 2]
    
    # Build phrases by finding multi-word sequences
    key_phrases = []
    for word, score in top_words[:max_phrases]:
        # Find the word in original text with context
        pattern = r'\b\w*' + re.escape(word) + r'\w*(?:\s+\w+){0,2}\b'
        matches = re.findall(pattern, text, re.IGNORECASE)
        if matches:
            key_phrases.append(matches[0].strip())
    
    return key_phrases[:max_phrases]

def generate_abstractive_summary(text, category, t5_model, t5_tokenizer, max_length=50):
    """
    Use T5 model to generate abstractive summary with category-aware prompting
    """
    # Category-specific prompts for better T5 output
    category_prompts = {
        "MEETING_RECAP": "Summarize this meeting note highlighting key decisions and action items: ",
        "SHOPPING_LIST": "Summarize this shopping list: ",
        "TRAVEL_LOG": "Summarize this travel experience: ",
        "WORK_PROJECT": "Summarize this project update: ",
        "TECHNICAL_REFERENCE": "Summarize this technical procedure: ",
        "CREATIVE_WRITING": "Summarize this creative piece: ",
        "FINANCE_LEGAL": "Summarize this financial note: ",
        "HEALTH_WELLNESS": "Summarize this health activity: ",
        "PERSONAL_DAILY_LIFE": "Summarize this daily activity: ",
        "EDUCATION_LEARNING": "Summarize this learning activity: ",
        "REMINDER": "Summarize this reminder: ",
        "HOME_FAMILY": "Summarize this family note: ",
        "FOOD_RECIPE": "Summarize this recipe: "
    }
    
    prompt = category_prompts.get(category, "Summarize: ") + text
    
    # Generate with T5
    inputs = t5_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=128)
    output_ids = t5_model.generate(
        **inputs,
        max_length=max_length,
        min_length=10,
        length_penalty=2.0,
        num_beams=4,
        no_repeat_ngram_size=3,  # Prevent repeating 3-grams
        repetition_penalty=1.5,   # Penalize repetition
        early_stopping=True
    )
    summary = t5_tokenizer.decode(output_ids[0], skip_special_tokens=True)
    
    return summary

def generate_enhanced_summary(note_text, category, t5_model, t5_tokenizer):
    """
    Generate comprehensive natural language summary with all advanced features
    """
    category_friendly = category.replace("_", " ").title()
    
    # 1. Extract named entities
    entities = extract_named_entities(note_text)
    
    # 2. Analyze sentiment
    sentiment_analysis = analyze_sentiment(note_text, category)
    
    # 3. Extract key phrases
    key_phrases = extract_key_phrases(note_text)
    
    # 4. Generate T5 abstractive summary
    abstractive_summary = generate_abstractive_summary(note_text, category, t5_model, t5_tokenizer)
    
    # 5. Parse context and content
    if ":" in note_text:
        context_part, content_part = note_text.split(":", 1)
        context_part = context_part.strip()
        content_part = content_part.strip()
    else:
        context_part = "General"
        content_part = note_text.strip()
    
    # 6. Detect content characteristics
    has_actions = any(word in content_part.lower() for word in 
                     ['assigned', 'completed', 'scheduled', 'reviewed', 'confirmed', 
                      'updated', 'created', 'mapped', 'departed', 'explored', 'documented'])
    
    is_list = content_part.count(',') >= 2
    is_long = len(content_part) > 100
    
    # 7. Build enhanced summary with multiple strategies
    
    # Strategy 1: Entity-rich summary
    if entities['people'] or entities['places']:
        entity_mentions = []
        if entities['people']:
            entity_mentions.append(f"involving {', '.join(entities['people'][:2])}")
        if entities['places']:
            entity_mentions.append(f"at {entities['places'][0]}")
        entity_context = ' '.join(entity_mentions)
    else:
        entity_context = ""
    
    # Strategy 2: Natural language summaries with embedded category context
    if has_actions and category == "MEETING_RECAP":
        if "assigned" in content_part.lower():
            action_summary = f"Meeting note about {context_part.lower()} covering task assignments and operational updates"
            if entity_context:
                action_summary += f" {entity_context}"
        else:
            action_summary = f"Meeting note discussing {context_part.lower()} and key decisions"
    
    elif category == "SHOPPING_LIST":
        items = [item.strip() for item in content_part.split(',')]
        item_count = len(items)
        if item_count > 3:
            first_items = ', '.join(items[:2])
            action_summary = f"Shopping list for {context_part.lower()} with {item_count} items including {first_items} and more"
        else:
            action_summary = f"Shopping list for {', '.join(items[:3])}"
    
    elif category == "TRAVEL_LOG":
        if entities['places']:
            action_summary = f"Travel log documenting journey to {entities['places'][0]} with experiences and observations"
        else:
            action_summary = f"Travel log entry about {context_part.lower()} capturing experiences and insights"
    
    elif category == "TECHNICAL_REFERENCE":
        if has_actions:
            procedure = key_phrases[0] if key_phrases else 'system maintenance'
            action_summary = f"Technical reference documenting step-by-step procedures for {context_part.lower()}"
        else:
            action_summary = f"Technical documentation for {context_part.lower()}"
    
    elif category == "CREATIVE_WRITING":
        action_summary = f"Creative writing piece: {abstractive_summary}"
    
    elif category == "WORK_PROJECT":
        if has_actions:
            action_summary = f"Work project note for {context_part.lower()} outlining milestones, tasks, and deliverables"
            if entity_context:
                action_summary += f" {entity_context}"
        else:
            action_summary = f"Work project tracking progress on {context_part.lower()}"
    
    elif category == "FINANCE_LEGAL":
        if entities['numbers']:
            action_summary = f"Financial note about {context_part.lower()} including {entities['numbers'][0] if entities['numbers'] else 'monetary transactions'}"
        else:
            action_summary = f"Financial record: {abstractive_summary}"
    
    elif category == "HEALTH_WELLNESS":
        activity_type = key_phrases[0] if key_phrases else "wellness activity"
        action_summary = f"Health and wellness note tracking {context_part.lower()} for personal improvement"
    
    elif category == "EDUCATION_LEARNING":
        if "completed" in content_part.lower() or "passed" in content_part.lower():
            action_summary = f"Learning milestone achieved: {context_part.lower()}"
        else:
            action_summary = f"Educational note about {context_part.lower()} and ongoing development"
    
    elif category == "REMINDER":
        action_summary = f"Reminder to {content_part[:60].lower()}"
    
    elif category == "PERSONAL_DAILY_LIFE":
        action_summary = f"Daily life note about {context_part.lower()}"
    
    elif category == "HOME_FAMILY":
        action_summary = f"Family and home note regarding {context_part.lower()}"
    
    elif category == "SELF_IMPROVEMENT":
        action_summary = f"Personal development note about {context_part.lower()}"
    
    elif category == "FOOD_RECIPE":
        if len(content_part) > 100:
            action_summary = f"Recipe with detailed instructions for {context_part.lower()}"
        else:
            action_summary = f"Recipe note for {context_part.lower()}"
    
    else:
        # Generic intelligent summary with multiple factors
        if is_long:
            # Use extractive + abstractive combination
            sentences = content_part.replace(',', '.').split('.')
            key_points = [s.strip() for s in sentences if len(s.strip()) > 10][:2]
            if key_phrases:
                action_summary = f"Note about {context_part.lower()}: {abstractive_summary}"
            else:
                combined = ' and '.join(key_points)
                action_summary = f"Detailed note about {context_part.lower()}: {combined[:80]}..."
        elif is_list:
            action_summary = f"Note documenting {context_part.lower()} with multiple items"
        else:
            action_summary = f"Note about {context_part.lower()}: {abstractive_summary}"
    
    # 8. Build comprehensive summary output
    summary_parts = {
        'primary': action_summary if 'action_summary' in locals() else f"{category_friendly}: {abstractive_summary}",
        'entities': entities,
        'sentiment': sentiment_analysis,
        'key_phrases': key_phrases,
        'abstractive': abstractive_summary
    }
    
    return summary_parts

# Demonstrate complete pipeline: classification + formatted summary generation
validation_examples = [
    "Shift meeting recap: aligned on mobile order staging flow, assigned Riley to monitor warming oven temps, noted need to reorder grande lids.",
    "Weekly grocery run: oat milk 3 cartons, almond butter 2 jars, spinach, blueberries, cold brew filters, biodegradable soap refill.",
    "Seattle to Portland trip: departed 8:15am on Amtrak Cascades, tasted seasonal lattes at Pioneer Courthouse store, noted crowd insights for merchandising deck.",
    "Poetic sketch: latte art swan mirrors sunrise commute, foam feathers tracing ambitions of the opening crew.",
    "Machine maintenance SOP: purge Mastrena lines for 6 seconds, run detergent cycle nightly, log pressure readings in Equipment Tracker tab."
]

validation_token_ids = vectorizer(np.array(validation_examples)).numpy()
validation_segment_ids = np.zeros_like(validation_token_ids)
validation_padding_mask = (validation_token_ids != 0).astype("int32")
validation_inputs = {
    "token_ids": validation_token_ids,
    "segment_ids": validation_segment_ids,
    "padding_mask": validation_padding_mask
}

validation_preds = classifier.predict(validation_inputs)

for i, note in enumerate(validation_examples):
    pred_class = int(np.argmax(validation_preds[i])) if validation_preds.ndim > 1 else int(validation_preds[i])
    predicted_category = NOTE_TYPES[pred_class]
    
    # Generate comprehensive enhanced summary
    summary_analysis = generate_enhanced_summary(note, predicted_category, model, tokenizer)
    
    print(f"Validation {i+1}:")
    print(f"  Raw Input: {note}")
    print(f"  Classifier: {predicted_category}")
    print(f"  Enhanced Summary: {summary_analysis['primary']}")
    print(f"  T5 Abstractive: {summary_analysis['abstractive']}")
    
    # Show extracted intelligence
    if any(summary_analysis['entities'].values()):
        print(f"  Entities Detected:")
        if summary_analysis['entities']['people']:
            print(f"    - People: {', '.join(summary_analysis['entities']['people'])}")
        if summary_analysis['entities']['places']:
            print(f"    - Places: {', '.join(summary_analysis['entities']['places'])}")
        if summary_analysis['entities']['numbers']:
            print(f"    - Metrics: {', '.join(summary_analysis['entities']['numbers'][:3])}")
    
    if summary_analysis['key_phrases']:
        print(f"  Key Phrases: {', '.join(summary_analysis['key_phrases'])}")
    
    print(f"  Sentiment: {summary_analysis['sentiment']['sentiment']} ({summary_analysis['sentiment']['tone']})")
    print(f"  âœ“ Advanced Features: NER + Sentiment + Key Phrases + Abstractive Summarization")
    print()

print("="*80)
print("MODEL VALIDATION COMPLETE")
print("="*80)


