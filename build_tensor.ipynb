# FLAN-T5-Small → TFLite (INT8 dynamic), CPU-only
# Fixes "untracked resource" by exporting methods on a tf.Module that owns the model.
%pip -q install "tensorflow==2.19.0" "tf-keras==2.19.0" \
                "transformers==4.44.2" "huggingface_hub>=0.24.0" \
                "numpy==2.0.2" "protobuf==5.29.1" "ml-dtypes>=0.5.0" \
                "datasets==3.1.0"

import os, zipfile, gc
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

import tensorflow as tf
try:
    tf.config.set_visible_devices([], "GPU")  # force CPU to avoid OOMs
except Exception:
    pass

from transformers import TFT5ForConditionalGeneration, T5Tokenizer
from datasets import Dataset
import numpy as np

MODEL_ID = "google/flan-t5-small"
FINETUNED_DIR = "flan_t5_small_type_first_finetuned"
OUT_DIR  = "flan_t5_small_type_first_tflite_min"
ENC_SAVED = os.path.join(OUT_DIR, "encoder_saved_model")
DEC_SAVED = os.path.join(OUT_DIR, "decoder_step_saved_model")
os.makedirs(OUT_DIR, exist_ok=True)

print("TF:", tf.__version__)

# Load tokenizer + TF model (from PyTorch weights)
tokenizer = T5Tokenizer.from_pretrained(MODEL_ID)
base_model = TFT5ForConditionalGeneration.from_pretrained(MODEL_ID, from_pt=True)

# Keep memory small
MAX_SRC_LEN = 64
MAX_TGT_LEN = 64
D_MODEL = base_model.config.d_model

# ---------- Load note → type-first summary dataset ----------
examples = [
    {
        "note": "Latte with oat milk, 2x vanilla pumps, add whipped cream. Customer noted dairy sensitivity and requested extra napkins.",
        "summary": "drink: latte (oat milk), modifiers: 2 vanilla, extras: whipped cream, requests: dairy sensitive, napkins=extra",
    },
    {
        "note": "Order for mobile pickup: two cake pops, one triple espresso over ice, double blended. Customer wants a sticker on each cup.",
        "summary": "drink: triple espresso (iced, double blend), food: 2 cake pops, requests: sticker per cup",
    },
    {
        "note": "Daily batch note - pastry case restock counted 14 croissants, 9 blueberry muffins, 6 lemon loaves.",
        "summary": "inventory: croissants=14, blueberry muffins=9, lemon loaves=6",
    },
    {
        "note": "Training reminder: review cold brew tapping steps with new partner Ava and sign completion log.",
        "summary": "task: training review, subject: cold brew tap, assignee: Ava, action: sign completion log",
    },
]

raw_dataset = Dataset.from_list(examples)
PROMPT = "summarize the note type and key counts: "

def preprocess_examples(batch):
    inputs = [PROMPT + note for note in batch["note"]]
    tokenized = tokenizer(
        inputs,
        max_length=MAX_SRC_LEN,
        truncation=True,
        padding="max_length",
    )
    targets = tokenizer(
        text_target=batch["summary"],
        max_length=MAX_TGT_LEN,
        truncation=True,
        padding="max_length",
    )
    labels = np.array(targets["input_ids"], dtype=np.int32)
    labels[labels == tokenizer.pad_token_id] = -100
    tokenized["labels"] = labels
    return tokenized

tokenized_dataset = raw_dataset.map(
    preprocess_examples,
    batched=True,
    remove_columns=raw_dataset.column_names,
)

BATCH_SIZE = 2
EPOCHS = 25
train_tf_dataset = tokenized_dataset.to_tf_dataset(
    columns=["input_ids", "attention_mask"],
    label_cols=["labels"],
    shuffle=True,
    batch_size=BATCH_SIZE,
)

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)
base_model.trainable = True
base_model.compile(optimizer=optimizer)
base_model.fit(train_tf_dataset, epochs=EPOCHS)

base_model.save_pretrained(FINETUNED_DIR)
tokenizer.save_pretrained(FINETUNED_DIR)

# Reload fine-tuned weights for export
model = TFT5ForConditionalGeneration.from_pretrained(FINETUNED_DIR)
model.trainable = False

# ---------- Export modules that TRACK the model variables ----------
class EncoderExport(tf.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model  # tracked

    @tf.function(
        input_signature=[
            tf.TensorSpec([None, MAX_SRC_LEN], tf.int32, name="input_ids"),
            tf.TensorSpec([None, MAX_SRC_LEN], tf.int32, name="attention_mask"),
        ]
    )
    def __call__(self, input_ids, attention_mask):
        enc = self.model.encoder(input_ids=input_ids,
                                 attention_mask=attention_mask,
                                 training=False)
        return {"encoder_last_hidden_state": enc.last_hidden_state}

class DecoderStepExport(tf.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model  # tracked

    @tf.function(
        input_signature=[
            tf.TensorSpec([None, 1], tf.int32, name="decoder_input_ids"),
            tf.TensorSpec([None, MAX_SRC_LEN, D_MODEL], tf.float32, name="encoder_last_hidden_state"),
            tf.TensorSpec([None, MAX_SRC_LEN], tf.int32, name="encoder_attention_mask"),
        ]
    )
    def __call__(self, decoder_input_ids, encoder_last_hidden_state, encoder_attention_mask):
        # In TF T5 (transformers 4.44.x), call model.decoder(...) directly
        dec_out = self.model.decoder(
            input_ids=decoder_input_ids,
            encoder_hidden_states=encoder_last_hidden_state,
            encoder_attention_mask=encoder_attention_mask,
            use_cache=False,
            training=False,
        )
        logits = self.model.lm_head(dec_out.last_hidden_state)  # (B,1,V)
        return {"logits": logits}

enc_export = EncoderExport(model)
dec_export = DecoderStepExport(model)

# Warm-up trace (tiny tensors)
_dummy_inp_ids = tf.zeros([1, MAX_SRC_LEN], dtype=tf.int32)
_dummy_attn    = tf.ones([1, MAX_SRC_LEN], dtype=tf.int32)
enc_hidden = enc_export(_dummy_inp_ids, _dummy_attn)["encoder_last_hidden_state"]
_ = dec_export(tf.zeros([1,1], tf.int32), enc_hidden, _dummy_attn)

# ---------- Save SavedModels (the modules track the variables) ----------
tf.saved_model.save(enc_export, ENC_SAVED)
tf.saved_model.save(dec_export, DEC_SAVED)

# ---------- Convert to TFLite (INT8 dynamic only for low RAM/size) ----------
def convert_dynamic(saved_model_dir, out_path):
    conv = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
    conv.optimizations = [tf.lite.Optimize.DEFAULT]
    tfl = conv.convert()
    with open(out_path, "wb") as f:
        f.write(tfl)

ENC_TFL = os.path.join(OUT_DIR, "encoder_int8_dynamic.tflite")
DEC_TFL = os.path.join(OUT_DIR, "decoder_step_int8_dynamic.tflite")
convert_dynamic(ENC_SAVED, ENC_TFL); gc.collect()
convert_dynamic(DEC_SAVED, DEC_TFL); gc.collect()

# ---------- Copy artifacts into Android assets ----------
from pathlib import Path
import shutil

repo_root = Path.cwd().resolve()
if not (repo_root / ".git").exists():
    for parent in repo_root.parents:
        if (parent / ".git").exists():
            repo_root = parent
            break

ASSET_DIR = (repo_root / "app" / "src" / "main" / "assets").resolve()
ASSET_DIR.mkdir(parents=True, exist_ok=True)
print(f"Asset directory ready: {ASSET_DIR}")

tokenizer_json = Path(FINETUNED_DIR) / "tokenizer.json"
if not tokenizer_json.exists():
    raise FileNotFoundError(f"tokenizer.json not found at {tokenizer_json}; ensure save_pretrained emitted it")

assets_to_copy = [
    (Path(ENC_TFL), ASSET_DIR / "encoder_int8_dynamic.tflite"),
    (Path(DEC_TFL), ASSET_DIR / "decoder_step_int8_dynamic.tflite"),
    (tokenizer_json, ASSET_DIR / "tokenizer.json"),
]

for src, dst in assets_to_copy:
    shutil.copyfile(src, dst)
    print(f"Staged {src} -> {dst}")

# ---------- Zip + download ----------
ARTIFACT_NAME = "flan_t5_small_type_first_tflite_min.zip"
# Android fetcher: publish/upload ARTIFACT_NAME to the usual /models/ path so mobile clients pull
# flan_t5_small_type_first_tflite_min.zip instead of the previous build.
zip_path = ARTIFACT_NAME
with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as z:
    for root, _, files in os.walk(OUT_DIR):
        for f in files:
            z.write(os.path.join(root, f), arcname=os.path.relpath(os.path.join(root, f), "."))

print("Encoder:", os.path.getsize(ENC_TFL)//1024, "KB")
print("Decoder:", os.path.getsize(DEC_TFL)//1024, "KB")
print("Zip:", os.path.getsize(zip_path)//1024, "KB")
print("Android fetcher artifact:", ARTIFACT_NAME)

try:
    from google.colab import files as _colab_files
except ModuleNotFoundError:
    _colab_files = None

if _colab_files is not None:
    _colab_files.download(zip_path)
    print(f"Triggered Colab download for {zip_path}")
else:
    print("google.colab not available; skipping browser download prompt.")
    print(f"Artifacts staged in: {ASSET_DIR}")

# Drive fallback (uncomment if mobile download prompt misbehaves)
# from google.colab import drive
# drive.mount('/content/drive', force_remount=True)
# !cp -v ${ARTIFACT_NAME} /content/drive/MyDrive/
