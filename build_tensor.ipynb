# FLAN-T5-Small → TFLite (INT8 dynamic), CPU-only
# Fixes "untracked resource" by exporting methods on a tf.Module that owns the model.
%pip -q install "tensorflow==2.19.0" "tf-keras==2.19.0" \
                "transformers==4.44.2" "huggingface_hub>=0.24.0" \
                "numpy==2.0.2" "protobuf==5.29.1" "ml-dtypes>=0.5.0" \
                "datasets==3.1.0"

import os, zipfile, gc
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

import tensorflow as tf
try:
    tf.config.set_visible_devices([], "GPU")  # force CPU to avoid OOMs
except Exception:
    pass

tf.config.run_functions_eagerly(True)

from transformers import TFT5ForConditionalGeneration, T5TokenizerFast
from datasets import Dataset
import numpy as np

MODEL_ID = "google/flan-t5-small"
FINETUNED_DIR = "flan_t5_small_type_first_finetuned"
OUT_DIR  = "flan_t5_small_type_first_tflite_min"
ENC_SAVED = os.path.join(OUT_DIR, "encoder_saved_model")
DEC_SAVED = os.path.join(OUT_DIR, "decoder_step_saved_model")
os.makedirs(OUT_DIR, exist_ok=True)

print("TF:", tf.__version__)

# Load tokenizer + TF model
tokenizer = T5TokenizerFast.from_pretrained(MODEL_ID)
base_model = TFT5ForConditionalGeneration.from_pretrained(MODEL_ID)

# Keep memory small
MAX_SRC_LEN = 64
MAX_TGT_LEN = 64
D_MODEL = base_model.config.d_model

# ---------- Load note → type-first summary dataset ----------
examples = [
    {
        "note": "Latte with oat milk, 2x vanilla pumps, add whipped cream. Customer noted dairy sensitivity and requested extra napkins.",
        "summary": "drink: latte (oat milk), modifiers: 2 vanilla, extras: whipped cream, requests: dairy sensitive, napkins=extra",
    },
    {
        "note": "Order for mobile pickup: two cake pops, one triple espresso over ice, double blended. Customer wants a sticker on each cup.",
        "summary": "drink: triple espresso (iced, double blend), food: 2 cake pops, requests: sticker per cup",
    },
    {
        "note": "Daily batch note - pastry case restock counted 14 croissants, 9 blueberry muffins, 6 lemon loaves.",
        "summary": "inventory: croissants=14, blueberry muffins=9, lemon loaves=6",
    },
    {
        "note": "Training reminder: review cold brew tapping steps with new partner Ava and sign completion log.",
        "summary": "task: training review, subject: cold brew tap, assignee: Ava, action: sign completion log",
    },
]

raw_dataset = Dataset.from_list(examples)
PROMPT = "summarize the note type and key counts: "

def preprocess_examples(batch):
    inputs = [PROMPT + note for note in batch["note"]]
    tokenized = tokenizer(
        inputs,
        max_length=MAX_SRC_LEN,
        truncation=True,
        padding="max_length",
    )
    targets = tokenizer(
        text_target=batch["summary"],
        max_length=MAX_TGT_LEN,
        truncation=True,
        padding="max_length",
    )
    labels = np.array(targets["input_ids"], dtype=np.int32)
    labels[labels == tokenizer.pad_token_id] = -100
    tokenized["labels"] = labels
    return tokenized

tokenized_dataset = raw_dataset.map(
    preprocess_examples,
    batched=True,
    remove_columns=raw_dataset.column_names,
)

BATCH_SIZE = 2
EPOCHS = 25
train_tf_dataset = tokenized_dataset.to_tf_dataset(
    columns=["input_ids", "attention_mask"],
    label_cols=["labels"],
    shuffle=True,
    batch_size=BATCH_SIZE,
)

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)
base_model.trainable = True
base_model.compile(optimizer=optimizer, run_eagerly=True)
base_model.fit(train_tf_dataset, epochs=EPOCHS)

base_model.save_pretrained(FINETUNED_DIR)
tokenizer.save_pretrained(FINETUNED_DIR, legacy_format=False)

# Reload fine-tuned weights for export
model = TFT5ForConditionalGeneration.from_pretrained(FINETUNED_DIR)
model.trainable = False

# ---------- Build exportable Keras models ----------
encoder_input_ids = tf.keras.Input([MAX_SRC_LEN], dtype=tf.int32, name="input_ids")
encoder_attention_mask = tf.keras.Input([MAX_SRC_LEN], dtype=tf.int32, name="attention_mask")
encoder_outputs = model.encoder(
    input_ids=encoder_input_ids,
    attention_mask=encoder_attention_mask,
    training=False,
)
encoder_model = tf.keras.Model(
    inputs=[encoder_input_ids, encoder_attention_mask],
    outputs={"encoder_last_hidden_state": encoder_outputs.last_hidden_state},
    name="encoder_export",
)

decoder_input_ids = tf.keras.Input([1], dtype=tf.int32, name="decoder_input_ids")
decoder_encoder_state = tf.keras.Input([MAX_SRC_LEN, D_MODEL], dtype=tf.float32, name="encoder_last_hidden_state")
decoder_attention_mask = tf.keras.Input([MAX_SRC_LEN], dtype=tf.int32, name="encoder_attention_mask")
decoder_outputs = model.decoder(
    input_ids=decoder_input_ids,
    encoder_hidden_states=decoder_encoder_state,
    encoder_attention_mask=decoder_attention_mask,
    use_cache=False,
    training=False,
)
decoder_logits = model.lm_head(decoder_outputs.last_hidden_state)
decoder_model = tf.keras.Model(
    inputs=[decoder_input_ids, decoder_encoder_state, decoder_attention_mask],
    outputs={"logits": decoder_logits},
    name="decoder_step_export",
)

# ---------- Save SavedModels ----------
tf.saved_model.save(encoder_model, ENC_SAVED)
tf.saved_model.save(decoder_model, DEC_SAVED)

# ---------- Convert to TFLite (INT8 dynamic only for low RAM/size) ----------
def convert_dynamic(saved_model_dir, out_path):
    print(f"Converting {saved_model_dir} -> {out_path}")
    conv = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
    conv.optimizations = [tf.lite.Optimize.DEFAULT]
    tfl = conv.convert()
    with open(out_path, "wb") as f:
        f.write(tfl)
    print(f"Wrote {out_path}")

ENC_TFL = os.path.join(OUT_DIR, "encoder_int8_dynamic.tflite")
DEC_TFL = os.path.join(OUT_DIR, "decoder_step_int8_dynamic.tflite")
convert_dynamic(ENC_SAVED, ENC_TFL); gc.collect()
convert_dynamic(DEC_SAVED, DEC_TFL); gc.collect()

# ---------- Copy artifacts into Android assets ----------
from pathlib import Path
import shutil

repo_root = Path.cwd().resolve()
if not (repo_root / ".git").exists():
    for parent in repo_root.parents:
        if (parent / ".git").exists():
            repo_root = parent
            break

ASSET_DIR = (repo_root / "app" / "src" / "main" / "assets").resolve()
ASSET_DIR.mkdir(parents=True, exist_ok=True)
print(f"Asset directory ready: {ASSET_DIR}")

tokenizer_json = Path(FINETUNED_DIR) / "tokenizer.json"
if not tokenizer_json.exists():
    raise FileNotFoundError(f"tokenizer.json not found at {tokenizer_json}; ensure save_pretrained emitted it")

assets_to_copy = [
    (Path(ENC_TFL), ASSET_DIR / "encoder_int8_dynamic.tflite"),
    (Path(DEC_TFL), ASSET_DIR / "decoder_step_int8_dynamic.tflite"),
    (tokenizer_json, ASSET_DIR / "tokenizer.json"),
]

for src, dst in assets_to_copy:
    shutil.copyfile(src, dst)
    print(f"Staged {src} -> {dst}")

# ---------- Zip + download ----------
ARTIFACT_NAME = "flan_t5_small_type_first_tflite_min.zip"
# Android fetcher: publish/upload ARTIFACT_NAME to the usual /models/ path so mobile clients pull
# flan_t5_small_type_first_tflite_min.zip instead of the previous build.
zip_path = ARTIFACT_NAME
with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as z:
    for root, _, files in os.walk(OUT_DIR):
        for f in files:
            z.write(os.path.join(root, f), arcname=os.path.relpath(os.path.join(root, f), "."))

print("Encoder:", os.path.getsize(ENC_TFL)//1024, "KB")
print("Decoder:", os.path.getsize(DEC_TFL)//1024, "KB")
print("Zip:", os.path.getsize(zip_path)//1024, "KB")
print("Android fetcher artifact:", ARTIFACT_NAME)

try:
    from google.colab import files as _colab_files
except ModuleNotFoundError:
    _colab_files = None

if _colab_files is not None:
    _colab_files.download(zip_path)
    print(f"Triggered Colab download for {zip_path}")
else:
    print("google.colab not available; skipping browser download prompt.")
    print(f"Artifacts staged in: {ASSET_DIR}")

# Drive fallback (uncomment if mobile download prompt misbehaves)
# from google.colab import drive
# drive.mount('/content/drive', force_remount=True)
# !cp -v ${ARTIFACT_NAME} /content/drive/MyDrive/
